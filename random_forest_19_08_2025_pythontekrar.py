# -*- coding: utf-8 -*-
"""Random_Forest_19.08.2025_pythontekrar.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YbInt1PQSVV3G5NnzNcX8RZp0RPnm9pe

# Random Forest
"""

from google.colab import drive
drive.mount('/content/drive')

"""# Import Necessary Libraries"""

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import math
from sklearn.metrics import mean_squared_error, mean_absolute_error, root_mean_squared_error, r2_score
from sklearn.model_selection import train_test_split, cross_val_score, KFold, GridSearchCV, validation_curve, cross_validate
from sklearn.preprocessing import LabelEncoder, RobustScaler, MinMaxScaler, StandardScaler
from sklearn import model_selection
from sklearn.ensemble  import RandomForestRegressor
import pickle
import joblib

# Settings
pd.set_option("display.max_columns", None)
pd.set_option("display.max_rows", None)
pd.set_option("display.width", 500)

import warnings
warnings.filterwarnings("ignore")

"""# Import Dataset"""

adv = pd.read_csv("/content/drive/MyDrive/Colab Notebooks/datasets/Hitters[1].csv")
df = adv.copy()
df.head()

"""# General Information About to Dataset"""

def check_df(dataframe, head=5):
    print(20*"#", "HEAD", 20*"#")
    print(dataframe.head(head))
    print(20*"#", "TAIL", 20*"#")
    print(dataframe.tail(head))
    print(20*"#", "SHAPE", 20*"#")
    print(dataframe.shape)
    print(20*"#", "TYPES", 20*"#")
    print(dataframe.dtypes)
    print(20*"#", "NA", 20*"#")
    print(dataframe.isnull().sum())
    print(20*"#", "QUARTILES", 20*"#")
    print(dataframe.describe([0.01, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.95, 0.99]).T)

check_df(df)

"""# Analysis of Catgeorical and Numerical Variables"""

def grab_col_names(dataframe, cat_th=10, car_th=20, report=False):
    # category
    cat_cols = [col for col in dataframe.columns if str(dataframe[col].dtypes) in  ["category", "object", "bool"]]
    num_but_cat = [col for col in dataframe.columns if dataframe[col].nunique() < cat_th and dataframe[col].dtypes in ["uint8", "int64", "float64"]]
    cat_but_car = [col for col in dataframe.columns if dataframe[col].nunique() > car_th and str(dataframe[col].dtypes) in ["category", "object"]]
    cat_cols = cat_cols + num_but_cat
    # numerical
    num_cols = [col for col in dataframe.columns if dataframe[col].dtypes in ["uint8", "int64", "float64"]]
    num_cols = [col for col in num_cols if col not in cat_cols]
    # report
    if report:
        print(f"Observation: {dataframe.shape[0]}")
        print(f"Variables: {dataframe.shape[1]}")
        print(f"Categrical Columns: {len(cat_cols)}")
        print(f"Numerical Columns: {len(num_cols)}")
        print(f"Categorical But Cardinality: {len(cat_but_car)}")
        print(f"Numerical But Categorical: {len(num_but_cat)}")
    return cat_cols, num_but_cat, cat_but_car, num_cols

cat_cols, num_but_cat, cat_but_car, num_cols = grab_col_names(df, report=True)

def num_summary(dataframe, col_name, plot=False):
    print(20*"#", col_name, 20*"#")
    quantiles = [0.01, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.95, 0.99]
    print(dataframe[col_name].describe(quantiles).T)
    if plot:
        dataframe[col_name].hist(bins=20)
        plt.xlabel(col_name)
        plt.ylabel(col_name)
        plt.show()

def num_summary_df(dataframe):
    cat_cols, num_but_cat, cat_but_car, num_cols = grab_col_names(dataframe)
    for col in num_cols:
        num_summary(dataframe, col, plot=True)

num_summary_df(df)

def plot_num_summary(dataframe):
    cat_cols, num_but_cat, cat_but_car, num_cols = grab_col_names(dataframe)
    num_plots = len(num_cols)
    rows = math.ceil(num_plots/2)
    cols = 2 if num_plots > 1 else 1
    plt.figure(figsize=(10*cols, 4*rows))
    for index, col in enumerate(num_cols):
        plt.subplot(rows, cols, index+1)
        plt.tight_layout()
        dataframe[col].hist(bins=20)
        plt.title(col)

plot_num_summary(df)

"""# Correlation Analysis"""

def high_correlated_cols(dataframe, corr_th=0.9, remove=False, plot=False):
    num_cols = [col for col in dataframe.columns if str(dataframe[col].dtypes) in ["uint8", "int64", "float64"]]
    corr = dataframe[num_cols].corr()
    corr_matrix = corr.abs()
    upper_triangle_matrix = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))
    drop_list = [col for col in upper_triangle_matrix.columns if any(upper_triangle_matrix[col] > corr_th)]
    if drop_list == []:
        print(20*"#", "After correlation analaysis, you do not need to remove variables", 20*"#")
    if remove:
        dataframe = dataframe.drop(drop_list, axis=1)
    if plot:
        num_cols = [col for col in dataframe.columns if str(dataframe[col].dtypes) in ["uint8", "int64", "float64"]]
        sns.set(rc={'figure.figsize': (18, 13)})
        sns.heatmap(dataframe[num_cols].corr(), cmap="RdBu", annot=True, fmt=".2f")
        plt.show()
    return drop_list

drop_list = high_correlated_cols(df, remove=False, plot=True)

drop_list

"""# Missing Value Analysis"""

df.isnull().sum()

def missing_value_table(dataframe):
  na_columns = [col for col in dataframe.columns if dataframe[col].isnull().sum() > 0]
  n_miss = dataframe[na_columns].isnull().sum().sort_values(ascending=False)
  ratio = (dataframe[na_columns].isnull().sum() / dataframe.shape[0] * 100).sort_values(ascending=False)
  missing_df = pd.concat([n_miss, np.round(ratio, 2)], axis=1, keys=["n_miss", "ratio"])
  print(missing_df)

missing_value_table(df)

def fill_na_with_median(dataframe):
  dataframe = dataframe.apply(lambda x: x.fillna(x.median()) if x.dtype not in ["category", "object", "bool"] else x, axis=0)
  return dataframe

df = fill_na_with_median(df)

df.isnull().sum().sum()

"""# Encoding"""

def label_encoder(dataframe, binary_col):
  labelencoder = LabelEncoder()
  dataframe[binary_col] = labelencoder.fit_transform(dataframe[binary_col])
  return dataframe

def label_encoder_df(dataframe):
  binary_cols = [col for col in dataframe.columns if dataframe[col].dtype not in ["int", "float"] and dataframe[col].nunique() == 2]
  for col in binary_cols:
    label_encoder(dataframe, col)
  return dataframe

def one_hot_encoding(dataframe):
  cat_cols = [col for col in dataframe.columns if  10 >= dataframe[col].nunique() > 2]
  dataframe = pd.get_dummies(dataframe, columns=cat_cols, drop_first=True)
  label_encoder_df(dataframe)
  return dataframe

df = one_hot_encoding(df)

df.head()

"""# Rnadom Forest

"""

def RF_Model(dataframe, target,  test_size=0.20, cv=10, results=False,save_model=False, plot_importance=False):
  X = dataframe.drop(target, axis=1)
  y = dataframe[target]
  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=1)
  rf_model = RandomForestRegressor(random_state=1).fit(X_train, y_train)

  if results:
    mse_train = mean_squared_error(y_train, rf_model.predict(X_train))
    mse_test = mean_squared_error(y_test, rf_model.predict(X_test))
    rmse_train = root_mean_squared_error(y_train, rf_model.predict(X_train))
    rmse_test = root_mean_squared_error(y_test, rf_model.predict(X_test))
    mae_train = mean_absolute_error(y_train, rf_model.predict(X_train))
    mae_test = mean_absolute_error(y_test, rf_model.predict(X_test))
    r2_train = r2_score(y_train, rf_model.predict(X_train))
    r2_test = r2_score(y_test, rf_model.predict(X_test))
    cv_result_mse = cross_validate(rf_model, X,y, cv=cv, scoring="neg_mean_squared_error")
    cv_result_rmse = cross_validate(rf_model, X, y, cv=cv, scoring="neg_root_mean_squared_error")
    print("MSE Train: ", "%.3f" % mse_train)
    print("MSE Test: ", "%.3f" % mse_test)
    print("RMSE Train: ", "%.3f" % rmse_train)
    print("RMSE Test: ", "%.3f" % rmse_test)
    print("MAE Train: ", "%.3f" % mae_train)
    print("MAE Test: ", "%.3f" % mae_test)
    print("R2 Train: ", "%.3f" % r2_train)
    print("R2 Test: ", "%.3f" % r2_test)
    print("Cross Validte MSE Score: ", "%.3f" % cv_result_mse["test_score"].mean())
    print("Cross Validte RMSE Score: ", "%.3f" % cv_result_rmse["test_score"].mean())

  if save_model:
    joblib.dump(rf_model, "rf_model.pkl")

  if plot_importance:
    feature_imp = pd.DataFrame({'Value': rf_model.feature_importances_, 'Feature': X.columns}).sort_values(by="Value", ascending=False)
    plt.figure(figsize=(8,6))
    sns.barplot(x="Value", y="Feature", data=feature_imp)
    plt.title("Feature Importance")
    plt.tight_layout()
    plt.savefig("importance.png")
    plt.show()

  return rf_model

rf_model = RF_Model(df, "Salary",  test_size=0.20, results=True,save_model=True, plot_importance=True)

"""# Load a Base Model and Then Prediction

"""

def laod_model(pklfile):
  model_disc = joblib.load(pklfile)
  return model_disc

model_disc = laod_model("rf_model.pkl")

new_data = df.sample(1)

new_data

new_data = new_data.drop("Salary", axis=1)

new_data

model_disc.predict(pd.DataFrame(new_data.values.tolist()[0]).T)[0]

"""# Model Tuning"""

def RF_Model_Tuning(dataframe, target,  test_size=0.20, cv=10, results=False,save_model=False, plot_importance=False):
  X = dataframe.drop(target, axis=1)
  y = dataframe[target]
  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=1)
  rf_model = RandomForestRegressor(random_state=1)
  rf_params = {
      "max_depth": list(range(1,6)),
      "max_features": [3,5,8,"auto"],
      "n_estimators": [100,200,500]
  }
  rf_cv_model= GridSearchCV(rf_model, rf_params, cv=cv, verbose=True, n_jobs=-1).fit(X_train, y_train)
  rf_model_tuned = rf_model.set_params(**rf_cv_model.best_params_, random_state=1).fit(X,y)

  if results:
    mse_train = mean_squared_error(y_train, rf_model_tuned.predict(X_train))
    mse_test = mean_squared_error(y_test, rf_model_tuned.predict(X_test))
    rmse_train = root_mean_squared_error(y_train, rf_model_tuned.predict(X_train))
    rmse_test = root_mean_squared_error(y_test, rf_model_tuned.predict(X_test))
    mae_train = mean_absolute_error(y_train, rf_model_tuned.predict(X_train))
    mae_test = mean_absolute_error(y_test, rf_model_tuned.predict(X_test))
    r2_train = r2_score(y_train, rf_model_tuned.predict(X_train))
    r2_test = r2_score(y_test, rf_model_tuned.predict(X_test))
    cv_result_mse = cross_validate(rf_model, X,y, cv=cv, scoring="neg_mean_squared_error")
    cv_result_rmse = cross_validate(rf_model, X, y, cv=cv, scoring="neg_root_mean_squared_error")
    print("MSE Train: ", "%.3f" % mse_train)
    print("MSE Test: ", "%.3f" % mse_test)
    print("RMSE Train: ", "%.3f" % rmse_train)
    print("RMSE Test: ", "%.3f" % rmse_test)
    print("MAE Train: ", "%.3f" % mae_train)
    print("MAE Test: ", "%.3f" % mae_test)
    print("R2 Train: ", "%.3f" % r2_train)
    print("R2 Test: ", "%.3f" % r2_test)
    print("Best Params: ", "%.3f", rf_cv_model.best_params_)
    print("Cross Validte MSE Score: ", "%.3f" % cv_result_mse["test_score"].mean())
    print("Cross Validte RMSE Score: ", "%.3f" % cv_result_rmse["test_score"].mean())


  if save_model:
    joblib.dump(rf_model_tuned, "rf_model_tuned.pkl")

  if plot_importance:
    feature_imp = pd.DataFrame({'Value': rf_model.feature_importances_, 'Feature': X.columns}).sort_values(by="Value", ascending=False)
    plt.figure(figsize=(8,6))
    sns.barplot(x="Value", y="Feature", data=feature_imp)
    plt.title("Feature Importance")
    plt.tight_layout()
    plt.savefig("importance.png")
    plt.show()

  return rf_model_tuned

rf_model_tuned = RF_Model_Tuning(df, "Salary",  test_size=0.20, cv=10, results=True,save_model=True, plot_importance=True)

"""# Load a Base Model and Then Prediction

"""

def laod_model(pklfile):
  model_disc = joblib.load(pklfile)
  return model_disc

model_disc = laod_model("rf_model.pkl")

new_data = df.sample(1)

new_data

new_data = new_data.drop("Salary", axis=1)

new_data

model_disc.predict(pd.DataFrame(new_data.values.tolist()[0]).T)[0]

"""# Plot Importance Features"""

X = df.drop("Salary", axis=1)
def plot_importance(model, features, num=len(X), save=False):
  feature_imp = pd.DataFrame({'Value': model.feature_importances_, 'Feature': features.columns}).sort_values(by="Value", ascending=False)
  plt.figure(figsize=(8,6))
  sns.barplot(x="Value", y="Feature", data=feature_imp[0:num])
  plt.title("Feature Importance")
  plt.tight_layout()

  if save:
    plt.savefig("importance.png")
  plt.show();
  return feature_imp

feature_imp = plot_importance(rf_model_tuned, X,save=True)

